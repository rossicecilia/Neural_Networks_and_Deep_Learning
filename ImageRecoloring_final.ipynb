{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRD3tXsU7MfG"
   },
   "source": [
    "# ***PART 1: DATA PROCESSING***\n",
    "- Download of the COCO Google Dataset\n",
    "- Arrange data inside Datasets and Dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec738Qgv62IO",
    "outputId": "1eeecefe-3f5b-4fe3-a681-f4f4650ffc59"
   },
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "#!pip install fiftyone\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone as fo\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "#Importing Libraries related to Pytorch\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchviz import make_dot\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dr_Shp8f67He",
    "outputId": "06f92b88-6c50-4140-e9c3-f4427e42bea6"
   },
   "outputs": [],
   "source": [
    "#### Importing the COCO Google Dataset with fiftyone\n",
    "\n",
    "SAMPLE_TRAIN = 10000\n",
    "SAMPLE_TEST = 500\n",
    "\n",
    "print(\"#\"*51 + \"IMPORTING COCO DATASET\" + \"#\"*51)\n",
    "print(\"#\"*51 + \"LOADING TRAINING SET\" +\"#\"*51)\n",
    "train_dataset = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split = \"train\",\n",
    "    max_samples = SAMPLE_TRAIN,\n",
    "    shuffle=True,\n",
    ")\n",
    "train_dataset.persistent = True\n",
    "\n",
    "\n",
    "print(\"#\"*51 + \"LOADING TEST SET\" + \"#\"*51)\n",
    "test_dataset = foz.load_zoo_dataset(\n",
    "    \"coco-2017\",\n",
    "    split=\"test\",\n",
    "    max_samples = SAMPLE_TEST,\n",
    ")\n",
    "test_dataset.persistent = True\n",
    "print('#'*51 + \"COCO DATASET IMPORTED\" + \"#\"*51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HV8p7Nm26_Hc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CocoDataset(Dataset):\n",
    "  #Constructor\n",
    "  def __init__(self, root:str, color_space: str=\"RGB\", size_limit=None, transform=None):\n",
    "    self.paths = [root+fname for fname in os.listdir(root)]\n",
    "    if size_limit != None:\n",
    "      self.paths = self.paths[:size_limit]\n",
    "    self.transform = transform\n",
    "    self.color_space = color_space\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.paths)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    sel_image_path = self.paths[index]\n",
    "    sel_image = cv2.imread(sel_image_path)\n",
    "\n",
    "    if self.transform != None:\n",
    "      sel_image = self.transform(sel_image)\n",
    "\n",
    "    #Firstly we transform the image into an np array\n",
    "    sel_image = np.array(sel_image)\n",
    "    #Since the images are saved in te BRG (not RGB) space\n",
    "    if self.color_space == \"RGB\":\n",
    "      sel_image = cv2.cvtColor(sel_image, cv2.COLOR_BGR2RGB) #original\n",
    "      bw_image = cv2.cvtColor(sel_image, cv2.COLOR_RGB2GRAY) #black and white\n",
    "      target_image = sel_image\n",
    "    elif self.color_space == \"Lab\":\n",
    "      sel_image = cv2.cvtColor(sel_image, cv2.COLOR_BGR2Lab)\n",
    "      bw_image = sel_image[:,:,[0]] #only the first channel --> brightness one (bw)\n",
    "      target_image = sel_image[:,:,[1,2]]\n",
    "\n",
    "    #At this point we transform everything into Tensor\n",
    "    target_image = transforms.ToTensor()(target_image)\n",
    "    bw_image = transforms.ToTensor()(bw_image)\n",
    "    #Then we do\n",
    "    target_image = 2.0 * target_image - 1.0 #we do some changes\n",
    "    bw_image = 2.0 * bw_image - 1.0\n",
    "    #Finally we return the (input image to the network = BW, and the label image = COLOR)\n",
    "    return bw_image, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhvYlBa670Pw",
    "outputId": "a9ccb82f-12a2-450c-c7b5-811175a854be"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 128 #size of images\n",
    "BATCH_SIZE = 8\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "#Transformations for the Images\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip()\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(), #why?\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE))\n",
    "])\n",
    "\n",
    "cars = True #using the car dataset or not\n",
    "#Creating the Datasets (RGB)\n",
    "if cars:\n",
    "    train_dataRGB = CocoDataset(root='C:/Users/alber/fiftyone/cars_dataset/train/', color_space = \"RGB\", size_limit = SAMPLE_TRAIN, transform=train_transforms)\n",
    "    test_dataRGB = CocoDataset(root='C:/Users/alber/fiftyone/cars_dataset/test/', color_space = 'RGB', size_limit = SAMPLE_TEST, transform=test_transforms)\n",
    "else:\n",
    "    train_dataRGB = CocoDataset(root='C:/Users/alber/fiftyone/coco-2017/train/data/', color_space = \"RGB\", size_limit = SAMPLE_TRAIN, transform=train_transforms)\n",
    "    test_dataRGB = CocoDataset(root='C:/Users/alber/fiftyone/coco-2017/test/data/', color_space = 'RGB', size_limit = SAMPLE_TEST, transform=test_transforms)\n",
    "\n",
    "#Creating the Dataloaders\n",
    "train_dl_RGB = DataLoader(train_dataRGB, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True, shuffle=True)\n",
    "test_dl_RGB = DataLoader(test_dataRGB, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True, shuffle=False)\n",
    "\n",
    "print(\"RGB DATASET\")\n",
    "print(f\"Training Set has: {len(train_dataRGB)} images\")\n",
    "print(f\"Test Set has: {len(test_dataRGB)} images\")\n",
    "\n",
    "\n",
    "#Creating the Datasets (Lab)\n",
    "if cars:\n",
    "    train_dataLAB = CocoDataset(root='C:/Users/alber/fiftyone/cars_dataset/train/', color_space = \"Lab\", size_limit = SAMPLE_TRAIN, transform=train_transforms)\n",
    "    test_dataLAB = CocoDataset(root='C:/Users/alber/fiftyone/cars_dataset/test/', color_space = 'Lab', size_limit = SAMPLE_TEST, transform=test_transforms)\n",
    "else:\n",
    "    train_dataLAB = CocoDataset(root='C:/Users/alber/fiftyone/coco-2017/train/data/', color_space = \"Lab\", size_limit = SAMPLE_TRAIN, transform=train_transforms)\n",
    "    test_dataLAB = CocoDataset(root='C:/Users/alber/fiftyone/coco-2017/test/data/', color_space = 'Lab', size_limit = SAMPLE_TEST, transform=test_transforms)\n",
    "#Creating the Dataloaders\n",
    "train_dl_LAB = DataLoader(train_dataLAB, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True, shuffle=True)\n",
    "test_dl_LAB = DataLoader(test_dataLAB, batch_size=BATCH_SIZE, num_workers=0, pin_memory=True, shuffle=False)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"LAB DATASET\")\n",
    "print(f\"Training Set has: {len(train_dataLAB)} images\")\n",
    "print(f\"Test Set has: {len(test_dataLAB)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OnHxF-V71X3"
   },
   "outputs": [],
   "source": [
    "def visualize(model, batch, color_space=\"RGB\", images_per_row=8, save = False ,filename = 'images_results'):\n",
    "    model.generator.eval()\n",
    "    with torch.no_grad():\n",
    "        x, label = batch\n",
    "        x = x.to(model.device)\n",
    "        output = model.generator(x)\n",
    "\n",
    "        x = x.cpu()\n",
    "        label = label.cpu()\n",
    "        output = output.detach().cpu()\n",
    "\n",
    "        x = (x + 1.0) / 2.0\n",
    "        output = (output + 1.0) / 2.0\n",
    "        label = (label + 1.0) / 2.0\n",
    "    model.generator.train()\n",
    "\n",
    "    if color_space == \"Lab\":\n",
    "        generated_images = torch.cat([x, output], dim=1)\n",
    "        true_images = torch.cat([x, label], dim=1)\n",
    "    else:\n",
    "        generated_images = output\n",
    "        true_images = label\n",
    "\n",
    "    num_images = generated_images.shape[0]\n",
    "    num_rows = int(np.ceil(num_images / images_per_row))\n",
    "\n",
    "    fig, axs = plt.subplots(3 * num_rows, images_per_row, figsize=(15, 8 * num_rows))\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "    for row in range(num_rows):\n",
    "        start_idx = row * images_per_row\n",
    "        end_idx = min((row + 1) * images_per_row, num_images)\n",
    "\n",
    "        for i in range(start_idx, end_idx):\n",
    "            inp, img, true_img = x[i], generated_images[i], true_images[i]\n",
    "\n",
    "            if color_space == \"Lab\":\n",
    "                img = transforms.ToPILImage()(img)\n",
    "                img = np.array(img)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_Lab2RGB)\n",
    "                true_img = transforms.ToPILImage()(true_img)\n",
    "                true_img = np.array(true_img)\n",
    "                true_img = cv2.cvtColor(true_img, cv2.COLOR_Lab2RGB)\n",
    "\n",
    "            axs[3 * row, i - start_idx].imshow(transforms.ToPILImage()(inp), cmap='gray')\n",
    "            axs[3 * row + 1, i - start_idx].imshow(transforms.ToPILImage()(img))\n",
    "            axs[3 * row + 2, i - start_idx].imshow(transforms.ToPILImage()(true_img))\n",
    "            axs[3 * row, i - start_idx].axis(\"off\")\n",
    "            axs[3 * row + 1, i - start_idx].axis(\"off\")\n",
    "            axs[3 * row + 2, i - start_idx].axis(\"off\")\n",
    "            \n",
    "    if save:\n",
    "        plt.savefig('C:/Users/alber/Desktop/MSc ICT Internet Multimedia Engineering/'+filename+'.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#The following function is used to save the output images instead of displaying them in the output and was used to produce the final report\n",
    "def save_outputs(model, batch, color_space=\"RGB\", save=True, folder_path='C:/Users/alber/Desktop/MSc ICT Internet Multimedia Engineering/outputs'):\n",
    "    model.generator.eval()\n",
    "    with torch.no_grad():\n",
    "        x, label = batch\n",
    "        x = x.to(model.device)\n",
    "        output = model.generator(x)\n",
    "\n",
    "        x = x.cpu()\n",
    "        label = label.cpu()\n",
    "        output = output.detach().cpu()\n",
    "\n",
    "        x = (x + 1.0) / 2.0\n",
    "        output = (output + 1.0) / 2.0\n",
    "        label = (label + 1.0) / 2.0\n",
    "    model.generator.train()\n",
    "\n",
    "    if color_space == \"Lab\":\n",
    "        generated_images = torch.cat([x, output], dim=1)\n",
    "        true_images = torch.cat([x, label], dim=1)\n",
    "    else:\n",
    "        generated_images = output\n",
    "        true_images = label\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        inp, img, true_img = x[i], generated_images[i], true_images[i]\n",
    "\n",
    "        if color_space == \"Lab\":\n",
    "            img = transforms.ToPILImage()(img)\n",
    "            img = np.array(img)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_Lab2RGB)\n",
    "            true_img = transforms.ToPILImage()(true_img)\n",
    "            true_img = np.array(true_img)\n",
    "            true_img = cv2.cvtColor(true_img, cv2.COLOR_Lab2RGB)\n",
    "\n",
    "        # Save images as PNG files in the specified folder\n",
    "        inp_filename = os.path.join(folder_path, f'input_{i}.png')\n",
    "        img_filename = os.path.join(folder_path, f'generated_{i}.png')\n",
    "        true_img_filename = os.path.join(folder_path, f'true_{i}.png')\n",
    "\n",
    "        transforms.ToPILImage()(inp).save(inp_filename)\n",
    "        transforms.ToPILImage()(img).save(img_filename)\n",
    "        transforms.ToPILImage()(true_img).save(true_img_filename)\n",
    "\n",
    "    print(f\"Images saved in {folder_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z6-lKdS79bI"
   },
   "source": [
    "# ***PART 2: MODELS***\n",
    "**Definition of the various models which will be used as generators:**\n",
    "- U-net\n",
    "- Autoencoder\n",
    "- ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fI7KDEnd9g4i"
   },
   "outputs": [],
   "source": [
    "REAL_LABELS = 1.\n",
    "FAKE_LABELS = 0.\n",
    "\n",
    "### Functions for the Initialization of the Model and its Weights\n",
    "\n",
    "def init_weights(net, init='norm', gain=0.02, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "    # function from the repository of the authors of the paper\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
    "            if init == 'norm':\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
    "            elif init == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif 'BatchNorm2d' in classname:\n",
    "            nn.init.normal_(m.weight.data, 1., gain)\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "\n",
    "    net.apply(init_func)\n",
    "    return net\n",
    "\n",
    "def init_model(model, device):\n",
    "    model = model.to(device)\n",
    "    model = init_weights(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAhsvE-5Eep1"
   },
   "source": [
    "**U-NET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1gJPbRb28haW",
    "outputId": "08cda809-15e2-4ac1-815d-586d8435552e"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, use_bias=False, use_dropout=True):\n",
    "        \n",
    "        #:param input_nc: number of input channels\n",
    "        #:param output_nc: number of output channels\n",
    "        #:param ngf: number of generator filters in the first convolutional layer\n",
    "        \n",
    "        super().__init__()\n",
    "        self.downrelu = nn.LeakyReLU(0.2, True)\n",
    "        self.uprelu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.drop_rate = 0.5 if use_dropout else 0.0\n",
    "\n",
    "        self.downconv1 = nn.Conv2d(input_nc, ngf, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downconv2 = nn.Conv2d(ngf, ngf*2, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn2   = nn.BatchNorm2d(ngf*2)\n",
    "        self.downconv3 = nn.Conv2d(ngf*2, ngf*4, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn3   = nn.BatchNorm2d(ngf*4)\n",
    "        self.downconv4 = nn.Conv2d(ngf*4, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn4   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv5 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn5   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv6 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn6   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv7 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.downbn7   = nn.BatchNorm2d(ngf*8)\n",
    "        self.downconv8 = nn.Conv2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "\n",
    "        self.upconv1   = nn.ConvTranspose2d(ngf*8, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn1     = nn.BatchNorm2d(ngf*8)\n",
    "        self.updrop1   = nn.Dropout(self.drop_rate)\n",
    "        self.upconv2   = nn.ConvTranspose2d(ngf*8*2, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn2     = nn.BatchNorm2d(ngf*8)\n",
    "        self.updrop2   = nn.Dropout(self.drop_rate)\n",
    "        self.upconv3   = nn.ConvTranspose2d(ngf*8*2, ngf*8, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn3     = nn.BatchNorm2d(ngf*8)\n",
    "        self.updrop3   = nn.Dropout(self.drop_rate)\n",
    "        self.upconv4   = nn.ConvTranspose2d(ngf*8*2, ngf*4, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn4     = nn.BatchNorm2d(ngf*4)\n",
    "        self.upconv5   = nn.ConvTranspose2d(ngf*4*2, ngf*2, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn5     = nn.BatchNorm2d(ngf*2)\n",
    "        self.upconv6   = nn.ConvTranspose2d(ngf*2*2, ngf, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "        self.upbn6     = nn.BatchNorm2d(ngf*1)\n",
    "        self.upconv7   = nn.ConvTranspose2d(ngf*2, output_nc, kernel_size=4, stride=2, padding=1, bias=use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1  = self.downconv1(x)                                        # input x is (input_nc) x 128 x 128\n",
    "        e2  = self.downbn2(self.downconv2(self.downrelu(e1)))          # input e1 is (ngf) x 64 x 64\n",
    "        e3  = self.downbn3(self.downconv3(self.downrelu(e2)))          # input e2 is (ngf * 2) x 32 x 32\n",
    "        e4  = self.downbn4(self.downconv4(self.downrelu(e3)))          # input e3 is (ngf * 4) x 16 x16\n",
    "        e5  = self.downbn5(self.downconv5(self.downrelu(e4)))          # input e4 is (ngf * 8) x 8 x 8\n",
    "        e6  = self.downbn6(self.downconv6(self.downrelu(e5)))          # input e5 is (ngf * 8) x 4 x 4\n",
    "        e7  = self.downconv7(self.downrelu(e6))                        # input e6 is (ngf * 8) x 2 x 2\n",
    "\n",
    "        d1_ = self.updrop1(self.upbn1(self.upconv1(self.uprelu(e7))))  # input e7 is (ngf * 8) x 1 x 1\n",
    "        d1  = torch.cat([d1_, e6], dim=1)\n",
    "        d2_ = self.updrop2(self.upbn2(self.upconv2(self.uprelu(d1))))  # input d1 is (ngf * 8 * 2) x 2 x 2\n",
    "        d2  = torch.cat([d2_, e5], dim=1)\n",
    "        d3_ = self.updrop3(self.upbn3(self.upconv3(self.uprelu(d2))))  # input d2 is (ngf * 8 * 2) x 4 x 4\n",
    "        d3  = torch.cat([d3_, e4], dim=1)\n",
    "        d4_ = self.upbn4(self.upconv4(self.uprelu(d3)))                # input d3 is (ngf * 8 * 2) x 8 x 8\n",
    "        d4  = torch.cat([d4_, e3], dim=1)\n",
    "        d5_ = self.upbn5(self.upconv5(self.uprelu(d4)))                # input d4 is (ngf * 8 * 2) x 16 x 16\n",
    "        d5  = torch.cat([d5_, e2], dim=1)\n",
    "        d6_ = self.upbn6(self.upconv6(self.uprelu(d5)))                # input d5 is (ngf * 4 * 2) x 32 x 32\n",
    "        d6  = torch.cat([d6_, e1], dim=1)\n",
    "        d7 =  self.upconv7(self.uprelu(d6))                            # input d6 is (ngf * 2 * 2) x 64 x 64\n",
    "                                                                       # input d7 is (ngf * 2) x 128 x 128\n",
    "        o1  = self.tanh(d7)\n",
    "        return o1\n",
    "\n",
    "net = Generator(1,3)\n",
    "x = torch.randn(1,1,128,128)\n",
    "\n",
    "\n",
    "#visualization = make_dot(net(x), params=dict(net.named_parameters()))\n",
    "#visualization.render(\"Unet_render\", format=\"png\", cleanup=True)\n",
    "\n",
    "print(net(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, use_bias=False, use_dropout=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, ngf, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(ngf, ngf*2, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Add more decoder layers with transpose convolutions\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, output_nc, kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return decoded\n",
    "\n",
    "\n",
    "net = Autoencoder(1,3)\n",
    "x = torch.randn(1,1,128,128)\n",
    "#visualization = make_dot(net(x), params=dict(net.named_parameters()))\n",
    "#visualization.render(\"Autoencoder_render\", format=\"png\", cleanup=True)\n",
    "print(net(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ResNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_bias=False):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=use_bias)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=use_bias)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=use_bias),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, num_residual_blocks=6, use_bias=False):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(input_nc, ngf, kernel_size=7, stride=1, padding=3, bias=use_bias)\n",
    "        self.bn1 = nn.BatchNorm2d(ngf)\n",
    "\n",
    "        # Downsampling\n",
    "        self.downsampling = nn.Sequential(\n",
    "            nn.Conv2d(ngf, ngf*2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            self.relu,\n",
    "            nn.Conv2d(ngf*2, ngf*4, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "            nn.BatchNorm2d(ngf*4),\n",
    "            self.relu\n",
    "        )\n",
    "\n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            ResidualBlock(ngf*4, ngf*4, stride=1, use_bias=use_bias) for _ in range(num_residual_blocks)\n",
    "        ])\n",
    "\n",
    "        # Upsampling\n",
    "        self.upsampling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            self.relu,\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, kernel_size=3, stride=2, padding=1, output_padding=1, bias=use_bias),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            self.relu\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Conv2d(ngf, output_nc, kernel_size=7, stride=1, padding=3, bias=use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.downsampling(out)\n",
    "\n",
    "        for block in self.residual_blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        out = self.upsampling(out)\n",
    "\n",
    "        out = self.output_layer(out)\n",
    "        out = self.tanh(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "net = GeneratorResNet(1,3)\n",
    "x = torch.randn(1,1,128,128)\n",
    "#visualization = make_dot(net(x), params=dict(net.named_parameters()))\n",
    "#visualization.render(\"ResNet_render\", format=\"png\", cleanup=True)\n",
    "print(net(x).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OQq83ly8z0r",
    "outputId": "0bc3d35c-0710-44ce-84a2-384044799566"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc, ndf=64):\n",
    "        \"\"\"\n",
    "        :param input_nc: number of input channels\n",
    "        :param ndf: number of discriminator filters in the first convolutional layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, True)\n",
    "\n",
    "        self.conv1    = nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=1) #input_nc + output_nc, ndf, 1, 1, 0 nell'articolo (pixelGan --> pix2pix)\n",
    "        self.conv2    = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1) #1, 1, 0\n",
    "        self.conv2_bn = nn.BatchNorm2d(ndf*2)\n",
    "        self.conv3    = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1) #ndf*2, 1, 1, 1, 0\n",
    "        self.conv3_bn = nn.BatchNorm2d(ndf*4) #no\n",
    "        self.conv4    = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=1, padding=1) #no\n",
    "        self.conv4_bn = nn.BatchNorm2d(ndf*8) #no\n",
    "        self.conv5    = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=1, padding=1) #no --> solo sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "      #print(\"Size input x: \", x.shape)\n",
    "      #print(\"Size after first conv: \", self.conv1(x).shape)\n",
    "      x = self.leaky_relu(self.conv1(x))\n",
    "      #print(\"Size after leaky relu 1: \", x.shape) #x is 3x256x256\n",
    "      #print(\"Size after second conv: \", self.conv2(x).shape)\n",
    "      x = self.leaky_relu(self.conv2_bn(self.conv2(x)))\n",
    "      #print(\"Size after leaky relu 2: \", x.shape)\n",
    "      #print(\"Size after third conv: \", self.conv3(x).shape)\n",
    "      x = self.leaky_relu(self.conv3_bn(self.conv3(x)))\n",
    "      #print(\"Size after leaky relu 3: \", x.shape)\n",
    "      #print(\"Size after 4th conv: \", self.conv4(x).shape)\n",
    "      x = self.leaky_relu(self.conv4_bn(self.conv4(x)))\n",
    "      #print(\"Size after leaky relu 4: \", x.shape)\n",
    "      x = self.conv5(x)  # No sigmoid since BCEWithLogitsLoss is used\n",
    "      #print(\"Size output (conv5): \", x.shape)\n",
    "      return x\n",
    "\n",
    "net = Discriminator(3)\n",
    "x = torch.randn(1,3,128,128)\n",
    "print(net(x).shape)\n",
    "\n",
    "PATCH_SIZE = net(x).shape[2] #now is 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "8CeqSF_U80qD",
    "outputId": "c4b746d0-a377-4bb8-edf7-9549287eaa75"
   },
   "outputs": [],
   "source": [
    "class GANModel(nn.Module):\n",
    "  def __init__(self, device, criterion, lr, betas, im_type=\"RGB\", ngf=64, ndf=64, use_bias=False, use_dropout=True, lambda_L1=0.0, gen_type='Unet'):\n",
    "    \"\"\"\n",
    "    :param im_type: a string (\"Lab\" or \"RGB\") indicating the image type\n",
    "    :param ngf: number of generator filters in the first convolutional layer\n",
    "    :param dgf: number of discriminator filters in the first convolutional layer\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.device = device\n",
    "    self.in_channels = 1\n",
    "    self.im_type = im_type\n",
    "    if self.im_type == \"RGB\":\n",
    "      self.out_channels = 3\n",
    "    else:\n",
    "      self.out_channels = 2\n",
    "\n",
    "    if gen_type == 'Unet':\n",
    "        self.generator = Generator(self.in_channels, self.out_channels).to(self.device)\n",
    "    elif gen_type == 'Autoencoder':\n",
    "        self.generator = Autoencoder(self.in_channels, self.out_channels).to(self.device)\n",
    "    elif gen_type == 'ResNet' :\n",
    "        self.generator = GeneratorResNet(self.in_channels, self.out_channels).to(self.device)\n",
    "        \n",
    "    self.discriminator = Discriminator(3).to(self.device)\n",
    "    self.generator.apply(init_weights)\n",
    "    self.discriminator.apply(init_weights)\n",
    "\n",
    "    (self.generator_lr , self.discriminator_lr) = lr\n",
    "    self.betas = betas\n",
    "\n",
    "    self.criterion = criterion\n",
    "    self.lambdaL1 = lambda_L1\n",
    "    self.L1loss = nn.L1Loss()\n",
    "\n",
    "    self.generator_optimizer = optim.Adam(self.generator.parameters(), lr=self.generator_lr, betas=betas)\n",
    "    self.discriminator_optimizer = optim.Adam(self.discriminator.parameters(), lr=self.discriminator_lr, betas=betas)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.generator(x)\n",
    "\n",
    "  def set_requires_grad(self, model, requires_grad=True):\n",
    "    for param in model.parameters():\n",
    "      param.requires_grad = requires_grad\n",
    "\n",
    "  def save(self, epoch, log, path=\"./checkpoint.pt\", download=True):\n",
    "    \"\"\"Saves state_dict for generator, discriminator and optimizers to path\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'epoch' : epoch,\n",
    "        'log' : log,\n",
    "        'generator_state_dict' : self.generator.state_dict(),\n",
    "        'discriminator_state_dict' : self.discriminator.state_dict(),\n",
    "        'generator_optimizer_state_dict' : self.generator_optimizer.state_dict(),\n",
    "        'discriminator_optimizer_state_dict' : self.discriminator_optimizer.state_dict()\n",
    "        }, path)\n",
    "\n",
    "  def load(self, path=\"./checkpoint.pt\"):\n",
    "    \"\"\"Loads state_dict for generator, discriminator and optimizers from path\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "    self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    self.generator_optimizer.load_state_dict(checkpoint['generator_optimizer_state_dict'])\n",
    "    self.discriminator_optimizer.load_state_dict(checkpoint['discriminator_optimizer_state_dict'])\n",
    "    return checkpoint['epoch'], checkpoint['log']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop with data logging ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STxCPpWE66C_"
   },
   "outputs": [],
   "source": [
    "def train_step(model, data, real_label, fake_label):\n",
    "    \"\"\"\n",
    "    What happens for each batch of the training dataloader = data\n",
    "    \"\"\"\n",
    "    x, label = data\n",
    "    x = x.to(model.device)\n",
    "    label = label.to(model.device)\n",
    "\n",
    "    ######### UPDATE D NETWORK ##########\n",
    "\n",
    "    if model.im_type == \"Lab\":\n",
    "      real_images = torch.cat([x, label], dim=1)\n",
    "    else:\n",
    "      real_images = label\n",
    "    real_images.to(model.device)\n",
    "\n",
    "    #Train with all the real batch\n",
    "    model.discriminator.train()\n",
    "    model.set_requires_grad(model.discriminator, True)\n",
    "    model.discriminator_optimizer.zero_grad()\n",
    "\n",
    "    output_D_real = model.discriminator(real_images).to(model.device) #passing real data to discriminator and save the output\n",
    "    label_D_real_images = torch.tensor(real_label).expand_as(output_D_real).to(model.device)\n",
    "    errorD_real = model.criterion(output_D_real, label_D_real_images) #comparing output with labels to compute the loss log(D(x))\n",
    "    errorD_real_item = errorD_real.item() \n",
    "\n",
    "    generator_output = model.generator(x).to(device)\n",
    "    if model.im_type == \"Lab\":\n",
    "      generated_images = torch.cat([x, generator_output], dim=1).to(model.device)\n",
    "    else:\n",
    "      generated_images = generator_output\n",
    "\n",
    "    output_D_fake = model.discriminator(generated_images.detach()).to(model.device) \n",
    "    label_D_fake_images = torch.tensor(fake_label).expand_as(output_D_fake).to(model.device)\n",
    "    errorD_fake = model.criterion(output_D_fake, label_D_fake_images)\n",
    "    errorD_fake_item = errorD_fake.item()\n",
    "\n",
    "    error_D = 0.5*(errorD_real + errorD_fake)\n",
    "    error_D = error_D.to(model.device)\n",
    "    error_D_item = error_D.item()\n",
    "    error_D.backward()\n",
    "    model.discriminator_optimizer.step()\n",
    "\n",
    "    ####### UPDATE G NETWORK ######\n",
    "    model.generator.train()\n",
    "    model.set_requires_grad(model.discriminator, False)\n",
    "    model.generator_optimizer.zero_grad()\n",
    "    output_D_fake = model.discriminator(generated_images).to(model.device)\n",
    "    loss_generated_images = model.criterion(output_D_fake, label_D_real_images)\n",
    "    loss_generated_images_item = loss_generated_images.item()\n",
    "    loss_L1 = model.L1loss(generator_output, label)\n",
    "    loss_L1_item = loss_L1.item()\n",
    "\n",
    "    error_G = loss_generated_images + loss_L1 * model.lambdaL1\n",
    "    error_G_item = loss_generated_images_item + loss_L1_item * model.lambdaL1\n",
    "    error_G.backward()\n",
    "\n",
    "    model.generator_optimizer.step()\n",
    "\n",
    "    return error_G_item, error_D_item\n",
    "\n",
    "def train_GAN(model, train_dl, epochs, checkpoint=None):\n",
    "    real_label = REAL_LABELS\n",
    "    fake_label = FAKE_LABELS\n",
    "\n",
    "    if checkpoint:\n",
    "        curr_ep, log = model.load(checkpoint)\n",
    "    else:\n",
    "        log = {'tr_generator_loss' : [], 'tr_discriminator_loss' : []}\n",
    "        curr_ep = 0\n",
    "\n",
    "    print(\"Starting TRAIN LOOP\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      ep_num = curr_ep + epoch + 1\n",
    "      b_n = 0\n",
    "      epoch_G_losses = []\n",
    "      epoch_D_losses = []\n",
    "      \n",
    "        \n",
    "      model.generator.train()\n",
    "      model.discriminator.train()\n",
    "\n",
    "      for batch in tqdm(train_dl, desc=f\"Epoch {epoch+1} / {epochs}\"):\n",
    "        G_loss, D_loss = train_step(model, batch, real_label, fake_label)\n",
    "        epoch_G_losses.append(G_loss) \n",
    "        epoch_D_losses.append(D_loss)\n",
    "          \n",
    "      avg_G_loss = np.mean(epoch_G_losses)\n",
    "      avg_D_loss = np.mean(epoch_D_losses)\n",
    "      print(f\"Epoch {ep_num}/{epochs}, Avg Loss G: {avg_G_loss}, Avg Loss D: {avg_D_loss}\")\n",
    "      log['tr_discriminator_loss'].append(avg_D_loss)\n",
    "      log['tr_generator_loss'].append(avg_G_loss)\n",
    "\n",
    "      wandb.log({\"epoch\":ep_num,\n",
    "                 \"generator_loss\": avg_G_loss,\n",
    "                 \"discriminator_loss\": avg_D_loss})\n",
    "\n",
    "      model.save(epoch=ep_num, log=log, path=f\"./checkpoint.pt\", download=False)\n",
    "      print(f\"SAVED CHECKPOINT EPOCH {ep_num}\")\n",
    "\n",
    "    return log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(generator_losses, discriminator_losses, filename='epochs_loss.png'):\n",
    "    epochs = range(1, len(generator_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, generator_losses, label='Generator Loss', marker='o')\n",
    "    plt.plot(epochs, discriminator_losses, label='Discriminator Loss', marker='o')\n",
    "\n",
    "    plt.title('Generator and Discriminator Losses Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('C:/Users/alber/Desktop/MSc ICT Internet Multimedia Engineering/'+filename+'.png')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXoWfKkvF06W",
    "outputId": "e3a4db6c-2225-411a-e249-44f091645682"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss() \n",
    "\n",
    "betas = [0.5, 0.999]\n",
    "lr = [2e-4, 2e-4]\n",
    "LAMBDA = 100. #in the article\n",
    "GEN_TYPE = 'ResNet'\n",
    "EPOCHS_l = [0] #list of number of epochs we want to train the net for\n",
    "\n",
    "IM_TYPE = 'RGB'  \n",
    "\n",
    "for ep in EPOCHS_l:\n",
    "    if cars:\n",
    "        model_saved_name = 'model_E' + str(ep) + '_I' + str(SAMPLE_TRAIN) + '_cars_' + GEN_TYPE + '_' + IM_TYPE\n",
    "        run = wandb.init(project='bw2rgb', notes = \"images in the training set: \" + str(SAMPLE_TRAIN) + '_cars_' + GEN_TYPE + '_' + IM_TYPE)\n",
    "    else:\n",
    "        model_saved_name = 'model_E' + str(ep) + '_I' + str(SAMPLE_TRAIN) +'_' + GEN_TYPE + '_' + IM_TYPE\n",
    "        run = wandb.init(project='bw2rgb', notes = \"images in the training set: \" + str(SAMPLE_TRAIN)+ '_' + GEN_TYPE + '_' + IM_TYPE)\n",
    "       \n",
    "    wandb.define_metric(\"epoch\")\n",
    "    wandb.define_metric(\"generator_loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"discriminator_loss\", step_metric=\"epoch\")\n",
    "    model = GANModel(device, criterion, lr, betas, im_type = IM_TYPE, lambda_L1=LAMBDA, gen_type=GEN_TYPE)\n",
    "    \n",
    "    if IM_TYPE == \"RGB\":\n",
    "        train_dl = train_dl_RGB\n",
    "        test_dl = test_dl_RGB\n",
    "    else:\n",
    "        train_dl = train_dl_LAB\n",
    "        test_dl = test_dl_LAB\n",
    "    log = train_GAN(model, train_dl, epochs=ep)\n",
    "    \n",
    "    G_losses = log['tr_generator_loss']\n",
    "    D_losses = log['tr_discriminator_loss']\n",
    "    plot_losses(G_losses, D_losses, filename=model_saved_name)\n",
    "\n",
    "    with open('C:/Users/alber/Desktop/MSc ICT Internet Multimedia Engineering/'+model_saved_name+'.txt', 'w') as output:\n",
    "        output.write(str(log))\n",
    "    torch.save(model, 'C:/Users/alber/Desktop/MSc ICT Internet Multimedia Engineering/'+model_saved_name+'.pt')\n",
    "\n",
    "    wandb.finish()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualzing results ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load(\"C:/Users/alber/Desktop/MSc ICT Internet Multimedia Engineering/NNDL/model_E45_I10000_cars_Unet_Lab.pt\")\n",
    "#model.eval()\n",
    "\n",
    "if IM_TYPE == \"RGB\":\n",
    "    train_dl = train_dl_RGB\n",
    "    test_dl = test_dl_RGB\n",
    "else:\n",
    "    train_dl = train_dl_LAB\n",
    "    test_dl = test_dl_LAB\n",
    "\n",
    "i = 0\n",
    "for batch in test_dl:\n",
    "  #save_outputs(model, batch, IM_TYPE, save = True)\n",
    "  visualize(model, batch, IM_TYPE, save = False, filename = 'results_' + str(i))\n",
    "  if i == 4:\n",
    "      break\n",
    "  i = i + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing final losses ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_loss(model, test_dl, L1_coeff = 0.0):\n",
    "  batch_losses = []\n",
    "  for batch in test_dl:\n",
    "    model.generator.eval()\n",
    "    model.discriminator.eval()\n",
    "    with torch.no_grad(): \n",
    "        x, label = batch\n",
    "        label = label.to(model.device)\n",
    "        x = x.to(model.device) \n",
    "        output = model.generator(x)\n",
    "        output = output.to(model.device)\n",
    "        \n",
    "\n",
    "        if model.im_type == \"Lab\":\n",
    "          generated_images = torch.cat([x, output], dim=1) #label and output are 2d, but we want a 3d image\n",
    "          true_images = torch.cat([x, label], dim=1)\n",
    "        else:\n",
    "          generated_images = output\n",
    "          true_images = label #3d already \n",
    "\n",
    "        #generated_images = generated_images.to(model.device)\n",
    "        discriminator_output = model.discriminator(generated_images)\n",
    "        label_GAN_loss = torch.tensor(1.).expand_as(discriminator_output).to(model.device)\n",
    "\n",
    "        #generated_images = generated_images.cpu()\n",
    "        L1_loss = nn.L1Loss()\n",
    "        loss_L1 = L1_loss(generated_images, true_images)\n",
    "        GAN_loss = model.criterion(discriminator_output, label_GAN_loss)\n",
    "        loss = GAN_loss + L1_coeff* loss_L1\n",
    "        loss.cpu()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "    model.generator.train()\n",
    "\n",
    "  return batch_losses\n",
    "\n",
    "\"\"\"\n",
    "#For the generic dataset:\n",
    "\n",
    "root_path = \"C:/Users/alber/Desktop/MSc ICT Internet Multimedia Engineering/generic_dataset/\"\n",
    "EPOCHS_fl = [15,30,45]\n",
    "im_count_l = [\"10000\", \"20000\"]\n",
    "\n",
    "losses_f = {}\n",
    "\n",
    "for im_count in im_count_l:\n",
    "    losses = np.array([])\n",
    "    for ep in EPOCHS_fl:\n",
    "        model_loc = root_path + \"E\" + str(ep) + \"_I\" + im_count + \"/model_E\" + str(ep) + \"_I\" + im_count +\".pt\"\n",
    "        model = torch.load(model_loc)\n",
    "        model.eval()\n",
    "    \n",
    "        test_losses = test_loss(model, test_dl, L1_coeff = LAMBDA)\n",
    "        average_test_loss = np.mean(test_losses)\n",
    "        losses = np.append(losses, average_test_loss)\n",
    "    losses_f[im_count] = losses\n",
    "\n",
    "print(losses_f)\n",
    "\n",
    "for im_count, loss_values in losses_f.items():\n",
    "    plt.plot(EPOCHS_fl, loss_values, label=f\"Images {im_count}\")\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Test Loss')\n",
    "plt.title('Test Loss vs Epochs for different dataset sizes')\n",
    "plt.legend()\n",
    "plt.savefig('C:/Users/alber/Desktop/MSc ICT Internet Multimedia Engineering/'+'test_losses'+'.png')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#For cars dataset:\n",
    "root_path = \"C:/Users/alber/Desktop/MSc ICT Internet Multimedia Engineering/cars_dataset/\"\n",
    "EPOCHS_fl = [15, 30, 45]\n",
    "im_count_l = [\"10000\"]\n",
    "\n",
    "losses_f = {}\n",
    "\n",
    "for im_count in im_count_l:\n",
    "    losses = np.array([])\n",
    "    for ep in EPOCHS_fl:\n",
    "        model_loc = root_path + \"E\" + str(ep) + \"_I\" + im_count + \"_cars/model_E\" + str(ep) + \"_I\" + im_count +\"_cars.pt\"\n",
    "        model = torch.load(model_loc)\n",
    "        model.eval()\n",
    "    \n",
    "        test_losses = test_loss(model, test_dl, L1_coeff = LAMBDA)\n",
    "        average_test_loss = np.mean(test_losses)\n",
    "        losses = np.append(losses, average_test_loss)\n",
    "    losses_f[im_count] = losses\n",
    "\n",
    "print(\"losses for \"+ GEN_TYPE + \"_\" + IM_TYPE + str(losses_f))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#for different architectures\n",
    "root_path = \"C:/Users/alber/Desktop/MSc ICT Internet Multimedia Engineering/NNDL/\"\n",
    "EPOCHS_fl = [45]\n",
    "im_count_l = [\"10000\"]\n",
    "\n",
    "losses_f = {}\n",
    "\n",
    "for im_count in im_count_l:\n",
    "    losses = np.array([])\n",
    "    for ep in EPOCHS_fl:\n",
    "        model_loc = root_path + \"model_E\" + str(ep) + \"_I\" + im_count + \"_cars_\" + GEN_TYPE + \"_\"+ IM_TYPE + \".pt\"\n",
    "        model = torch.load(model_loc)\n",
    "        model.eval()\n",
    "    \n",
    "        test_losses = test_loss(model, test_dl, L1_coeff = LAMBDA)\n",
    "        average_test_loss = np.mean(test_losses)\n",
    "        losses = np.append(losses, average_test_loss)\n",
    "    losses_f[im_count] = losses\n",
    "\n",
    "print(\"losses for \"+ GEN_TYPE + \"_\" + IM_TYPE + str(losses_f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
